
# Select and Deploy an Open-Source Model


Currently, Agent Platform supports thirty-plus open-source models and provides them as a service. If you select the Kore-hosted model, you can optimize it before deployment.

To select and deploy a model, follow these steps:

1. Go to **Models** > **Open-source models** and click **Deploy a model**. 
<img src="../images/deploy-a-model.png" alt="Deploy a Model" title="Deploy a Model" style="border: 1px solid gray; zoom:80%;">


2. The **Deploy** dialog is displayed. In the **General details** section:
    * If you choose **Kore-hosted models**, select the **model** from the dropdown menu. Add a **Description** and provide **tags** to ease the search for the model and click **Next**.
    
      <img src="../images/image8.png" alt=" " title=" " style="border: 1px solid gray; zoom:80%;">

The supported models and their variants are given below:

<table>
<tr>
<td><strong>MODEL</strong>
   </td>
   <td><strong>VARIANT</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Amazon</strong>
   </td>
   <td>amazon/MistralLite
   </td>
  </tr>
  <tr>
   <td><strong>Argilla</strong>
   </td>
   <td>
<ul>

<li>argilla/notus-7b-v1</li>

<li>argilla/notux-8x7b-v1</li>
</ul>
   </td>
  </tr>
  <tr>
   <td><strong>Eleutherai</strong>
   </td>
   <td>
<ul>

<li>EleutherAI/gpt-j-6b</li>

<li>EleutherAI/gpt-neo-1.3B</li>

<li>EleutherAI/gpt-neo-125m</li>

<li>EleutherAI/gpt-neo-2.7B</li>

<li>EleutherAI/gpt-neox-20b</li>
</ul>
   </td>
  </tr>
  <tr>
   <td><strong>Facebook</strong>
   </td>
   <td>
<ul>

<li>facebook/opt-1.3b</li>

<li>facebook/opt-2.7b</li>

<li>facebook/opt-350m</li>

<li>facebook/opt-6.7b</li>
</ul>
   </td>
  </tr>
  <tr>
   <td><strong>Google</strong>
   </td>
   <td>
<ul>

<li>google/flan-t5-base</li>

<li>google/flan-t5-large</li>

<li>google/flan-t5-small</li>

<li>google/flan-t5-xl</li>

<li>google/flan-t5-xxl</li>

<li>google/gemma-2-27b-it</li>

<li>google/gemma-2-9b-it</li>

<li>google/gemma-2b</li>

<li>google/gemma-2b-it</li>

<li>google/gemma-7b</li>

<li>google/gemma-7b-it</li>
</ul>
   </td>
  </tr>
  <tr>
   <td><strong>Helsinki-nlp</strong>
   </td>
   <td>Helsinki-NLP/opus-mt-es-en
   </td>
  </tr>
  <tr>
   <td><strong>Huggingfaceh4</strong>
   </td>
   <td>
<ul>

<li>HuggingFaceH4/zephyr-7b-alpha</li>

<li>HuggingFaceH4/zephyr-7b-beta</li>
</ul>
   </td>
  </tr>
  <tr>
   <td><strong>Meta-llama</strong>
   </td>
   <td>
<ul>

<li>meta-llama/Llama-2-13b-hf</li>

<li>meta-llama/Llama-2-7b-hf</li>

<li>meta-llama/Llama-3.2-1B</li>

<li>meta-llama/Llama-3.2-1B-Instruct</li>

<li>meta-llama/Llama-3.2-3B</li>

<li>meta-llama/Llama-3.2-3B-Instruct</li>

<li>meta-llama/Meta-Llama-3-8B</li>

<li>meta-llama/Meta-Llama-3-8B-Instruct</li>

<li>meta-llama/Meta-Llama-3.1-8B</li>

<li>meta-llama/Meta-Llama-3.1-8B-Instruct</li>
</ul>
   </td>
  </tr>
  <tr>
   <td><strong>Microsoft</strong>
   </td>
   <td>
<ul>

<li>microsoft/phi-1</li>

<li>microsoft/phi-1_5</li>

<li>microsoft/phi-2</li>

<li>microsoft/Phi-3-medium-128k-instruct</li>

<li>microsoft/Phi-3-medium-4k-instruct</li>

<li>microsoft/Phi-3-mini-128k-instruct</li>

<li>microsoft/Phi-3-mini-4k-instruct</li>
</ul>
   </td>
  </tr>
  <tr>
   <td><strong>Mistralai</strong>
   </td>
   <td>
<ul>

<li>mistralai/Mistral-7B-Instruct-v0.1</li>

<li>mistralai/Mistral-7B-Instruct-v0.2</li>

<li>mistralai/Mistral-7B-Instruct-v0.3</li>

<li>mistralai/Mistral-7B-v0.1</li>

<li>mistralai/Mistral-Nemo-Instruct-2407</li>

<li>mistralai/Mixtral-8x7B-Instruct-v0.1</li>

<li>mistralai/Mixtral-8x7B-v0.1</li>
</ul>
   </td>
  </tr>
  <tr>
   <td><strong>OpenAI</strong>
   </td>
   <td>GPT2
   </td>
  </tr>
  <tr>
   <td><strong>OpenAI Community</strong>
   </td>
   <td>
<ul>

<li>openai-community/gpt2-large</li>

<li>openai-community/gpt2-medium</li>

<li>openai-community/gpt2-xl</li>
</ul>
   </td>
  </tr>
  <tr>
   <td><strong>Stability AI</strong>
   </td>
   <td>
<ul>

<li>stabilityai/stable-diffusion-2-1</li>

<li>stabilityai/stable-diffusion-xl-base-1.0</li>

<li>stable-diffusion-v1-5/stable-diffusion-v1-5
 <strong>Stable Diffusion</strong></li>

<li>stabilityai/stable-diffusion-xl-base-1.0</li>

<li> stabilityai/stable-diffusion-2-1</li>

<li> stable-diffusion-v1-5/stable-diffusion-v1-5
<p>
<strong>Note</strong>: </li>

<li>Stable Diffusion is not available in the Prompt Studio.</li>

<li>This variant is allowed only in the text-to-image node and no other node.</li>
</ul>
   </td>
  </tr>
  <tr>
   <td><strong>T5</strong>
   </td>
   <td>
<ul>

<li>t5-base</li>

<li>t5-large</li>

<li>t5-small</li>
</ul>
   </td>
  </tr>
  <tr>
   <td><strong>Tiiuae</strong>
   </td>
   <td>
<ul>

<li>tiiuae/falcon-40b</li>

<li>tiiuae/falcon-40b-instruct</li>

<li>tiiuae/falcon-7b</li>
</ul>
   </td>
  </tr>
</table>


  * If you choose to **Import from Hugging Face**, select the **Hugging Face connection** type from the dropdown and paste the **model name**.
   For more information about connecting to your Hugging Face account, see[ How to Connect to your Hugging Face Account](../../settings/integrations/enable-hugging-face.md).

    !!! note

        In the case of public mode, selecting a connection is not necessary.  


    <img src="../images/image7.png" alt=" " title=" " style="border: 1px solid gray; zoom:80%;">

  
<ol start="3"><li>Based on the selected Kore-hosted model, the Optimization section is displayed. Choose the optimization option as required and then click <b>Next</b>. <a href="https://docs.kore.ai/agent-platform/models/open-source-models/model-optimization/" target="_blank">Learn more</a>.</li></ol>

* **Skip optimization**: It skips the model optimization.
* **CTranslate2**: Select Quantization from the dropdown menu if applicable. [Learn more](model-optimization.md#ctranslate2). 
* **vLLM**: Select Quantization from the dropdown menu if applicable. [Learn more](model-optimization.md#vllm).  


<img src="../images/image1.png" alt=" " title=" " style="border: 1px solid gray; zoom:80%;">   


<ol start="4"><li>In the <b>Parameters</b> section:</li>  

<ul><li>Select the Sampling <b>Temperature</b> to use for deployment.</li>
<li>Select the <b>Maximum length</b>, which implies the maximum number of tokens to generate.</li>
<li>Select the <b>Top p</b>, an alternative to sampling with the temperature where the model considers the results of the tokens with top_p probability mass.</li>
<li>Select the <b>Top k</b> value, the highest probability vocabulary tokens to keep for top-k-filtering.</li>
<li>Enter the <b>Stop sequences</b>, which tells the model when to stop generating further tokens.</li>
<li>Enter the <b>Inference batch size</b>, which is used to batch the concurrent requests at the time of model inferencing.</li>
<li>Select the <b>Min replicas</b>, which indicates the minimum number of model replicas to be deployed.</li>
<li>Select the <b>Max replicas</b>, which indicates the maximum number of model replicas to auto-scale.</li>
<li>Select the <b>Scale-up delay (in seconds)</b>, which indicates how long to wait before scaling up replicas.</li>
<li>Select the <b>Scale down replicas (in seconds)</b>, which indicates how long to wait before scaling down replicas.</li>  

<img src="../images/image2.png" alt=" " title=" " style="border: 1px solid gray; zoom:80%;"></ul>
<li>Click <b>Next</b>.</li>
<li>In the Hardware section, select the required hardware for deployment from the dropdown menu and click <b>Next</b>. 
<img src="../images/image6.png" alt=" " title=" " style="border: 1px solid gray; zoom:80%;"></li>

<li>In the <b>Review</b> section, verify all the details before starting the fine-tuning. To modify previous steps, click <b>Back</b>. Go through the terms and conditions and tick the checkbox <b>I accept all the Terms and Conditions</b>.

<img src="../images/image3.png" alt=" " title=" " style="border: 1px solid gray; zoom:80%;"></li>
<li>Click <b>Deploy</b>.</li></ol>

      
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You will be charged for deployment and inferencing-related costs for each open-source model. </p></div> 

If you have selected optimization, the model optimization starts, and the status changes to “Optimization”. If not, the model is deployed. After deployment, the status changes to "Deployed." You can now use this model across Agent Platform and externally.

Hover over the deployed model to view **more** icons (three dots) which provide access to the model **API endpoint** and **Configurations**. Selecting the API endpoint option shows the API endpoint, deployment history, API keys, and other details. Selecting the Configuration option allows you to add a description, tags, and deploy or delete the model.


<img src="../images/image4.png" alt=" " title=" " style="border: 1px solid gray; zoom:80%;">

### Re-deploy a Deployed Model

After the initial deployment, if you wish to update the model’s parameters, hardware, or both, you must redeploy the updated model.


To re-deploy a deployed model, follow these steps:


1. Go to **Models** > **Open-source models** and select a **model** to redeploy.
2. Click the **Deploy model**. The **Model Configuration** page is displayed.
3. Modify the required fields and click the **Deploy**. Once deployed, the status changes to “Deployed”.
